{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "8ooaUKuY10IF"
      },
      "outputs": [
        {
          "ename": "SyntaxError",
          "evalue": "invalid syntax (2252928269.py, line 1)",
          "output_type": "error",
          "traceback": [
            "\u001b[0;36m  Cell \u001b[0;32mIn[31], line 1\u001b[0;36m\u001b[0m\n\u001b[0;31m    pip install import-ipynb\u001b[0m\n\u001b[0m        ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ],
      "source": [
        "pip install import-ipynb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {},
      "outputs": [],
      "source": [
        "import import_ipynb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 84,
      "metadata": {},
      "outputs": [],
      "source": [
        "import Email\n",
        "import NER\n",
        "import Scheduling\n",
        "import Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "qcAF0vbsudFi"
      },
      "outputs": [
        {
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'regex'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[37], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mregex\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mre\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'regex'"
          ]
        }
      ],
      "source": [
        "# import pandas as pd\n",
        "# import re\n",
        "# import torch\n",
        "# import numpy as np\n",
        "\n",
        "# import transformers\n",
        "# from transformers import BertTokenizer\n",
        "# from torch import nn\n",
        "# from transformers import BertModel\n",
        "# from torch.optim import Adam\n",
        "# from tqdm import tqdm\n",
        "# import nltk\n",
        "# nltk.download('punkt')\n",
        "# from nltk.tag.stanford import StanfordNERTagger\n",
        "# from nltk.tokenize import word_tokenize\n",
        "# from sklearn import metrics\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zvvwb4Dowwrr"
      },
      "source": [
        "  GET EMAIL AND RELEVANT DETAILS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2BCcREyUwasW"
      },
      "outputs": [],
      "source": [
        "#get the service and extract email components\n",
        "service = Email.getService()\n",
        "email_details = Email.getEmails(service)\n",
        "userInput = email_details[\"Message\"]\n",
        "print(userInput)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YH_ltIZsxDXe"
      },
      "source": [
        "Install neccessary packages for training and evaluating the BERT classifier "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 123,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting transformers\n",
            "  Downloading transformers-4.29.2-py3-none-any.whl (7.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.1/7.1 MB\u001b[0m \u001b[31m27.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hCollecting filelock\n",
            "  Downloading filelock-3.12.0-py3-none-any.whl (10 kB)\n",
            "Collecting huggingface-hub<1.0,>=0.14.1\n",
            "  Downloading huggingface_hub-0.15.1-py3-none-any.whl (236 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m236.8/236.8 kB\u001b[0m \u001b[31m26.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /Users/jacklinewambui/Library/Python/3.11/lib/python/site-packages (from transformers) (1.24.3)\n",
            "Requirement already satisfied: packaging>=20.0 in /Users/jacklinewambui/Library/Python/3.11/lib/python/site-packages (from transformers) (23.1)\n",
            "Collecting pyyaml>=5.1\n",
            "  Downloading PyYAML-6.0-cp311-cp311-macosx_11_0_arm64.whl (167 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m167.5/167.5 kB\u001b[0m \u001b[31m22.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /opt/homebrew/lib/python3.11/site-packages (from transformers) (2023.6.3)\n",
            "Requirement already satisfied: requests in /opt/homebrew/lib/python3.11/site-packages (from transformers) (2.31.0)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1\n",
            "  Downloading tokenizers-0.13.3-cp311-cp311-macosx_12_0_arm64.whl (3.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.9/3.9 MB\u001b[0m \u001b[31m43.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /opt/homebrew/lib/python3.11/site-packages (from transformers) (4.65.0)\n",
            "Collecting fsspec\n",
            "  Downloading fsspec-2023.5.0-py3-none-any.whl (160 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m160.1/160.1 kB\u001b[0m \u001b[31m16.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting typing-extensions>=3.7.4.3\n",
            "  Downloading typing_extensions-4.6.3-py3-none-any.whl (31 kB)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/homebrew/lib/python3.11/site-packages (from requests->transformers) (3.1.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /opt/homebrew/lib/python3.11/site-packages (from requests->transformers) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/homebrew/lib/python3.11/site-packages (from requests->transformers) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /opt/homebrew/lib/python3.11/site-packages (from requests->transformers) (2023.5.7)\n",
            "Installing collected packages: tokenizers, typing-extensions, pyyaml, fsspec, filelock, huggingface-hub, transformers\n",
            "Successfully installed filelock-3.12.0 fsspec-2023.5.0 huggingface-hub-0.15.1 pyyaml-6.0 tokenizers-0.13.3 transformers-4.29.2 typing-extensions-4.6.3\n",
            "\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.1.2\u001b[0m\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3.11 -m pip install --upgrade pip\u001b[0m\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 129,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torch in /opt/homebrew/lib/python3.11/site-packages (2.0.1)\n",
            "Requirement already satisfied: filelock in /opt/homebrew/lib/python3.11/site-packages (from torch) (3.12.0)\n",
            "Requirement already satisfied: typing-extensions in /opt/homebrew/lib/python3.11/site-packages (from torch) (4.6.3)\n",
            "Requirement already satisfied: sympy in /opt/homebrew/lib/python3.11/site-packages (from torch) (1.12)\n",
            "Requirement already satisfied: networkx in /opt/homebrew/lib/python3.11/site-packages (from torch) (3.1)\n",
            "Requirement already satisfied: jinja2 in /opt/homebrew/lib/python3.11/site-packages (from torch) (3.1.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /opt/homebrew/lib/python3.11/site-packages (from jinja2->torch) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /opt/homebrew/lib/python3.11/site-packages (from sympy->torch) (1.3.0)\n",
            "\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.1.2\u001b[0m\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3.11 -m pip install --upgrade pip\u001b[0m\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "pip install torch\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 134,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "import torch\n",
        "import numpy as np\n",
        "import transformers\n",
        "from transformers import BertTokenizer\n",
        "from torch import nn\n",
        "from transformers import BertModel\n",
        "from torch.optim import Adam\n",
        "from tqdm import tqdm\n",
        "import nltk\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 145,
      "metadata": {},
      "outputs": [
        {
          "ename": "ParserError",
          "evalue": "Error tokenizing data. C error: Expected 1 fields in line 3, saw 2\n",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mParserError\u001b[0m                               Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[145], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m datapath \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/Users/jacklinewambui/Desktop/finalProject/CS72 Project_Emails_training_dataset.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 2\u001b[0m df\u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdatapath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlatin1\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m df \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39mdropna()\n\u001b[1;32m      4\u001b[0m df\u001b[38;5;241m.\u001b[39misnull()\u001b[38;5;241m.\u001b[39mvalues\u001b[38;5;241m.\u001b[39many()\n",
            "File \u001b[0;32m~/Library/Python/3.11/lib/python/site-packages/pandas/io/parsers/readers.py:912\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m    899\u001b[0m kwds_defaults \u001b[39m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m    900\u001b[0m     dialect,\n\u001b[1;32m    901\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    908\u001b[0m     dtype_backend\u001b[39m=\u001b[39mdtype_backend,\n\u001b[1;32m    909\u001b[0m )\n\u001b[1;32m    910\u001b[0m kwds\u001b[39m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m--> 912\u001b[0m \u001b[39mreturn\u001b[39;00m _read(filepath_or_buffer, kwds)\n",
            "File \u001b[0;32m~/Library/Python/3.11/lib/python/site-packages/pandas/io/parsers/readers.py:583\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    580\u001b[0m     \u001b[39mreturn\u001b[39;00m parser\n\u001b[1;32m    582\u001b[0m \u001b[39mwith\u001b[39;00m parser:\n\u001b[0;32m--> 583\u001b[0m     \u001b[39mreturn\u001b[39;00m parser\u001b[39m.\u001b[39;49mread(nrows)\n",
            "File \u001b[0;32m~/Library/Python/3.11/lib/python/site-packages/pandas/io/parsers/readers.py:1704\u001b[0m, in \u001b[0;36mTextFileReader.read\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m   1697\u001b[0m nrows \u001b[39m=\u001b[39m validate_integer(\u001b[39m\"\u001b[39m\u001b[39mnrows\u001b[39m\u001b[39m\"\u001b[39m, nrows)\n\u001b[1;32m   1698\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1699\u001b[0m     \u001b[39m# error: \"ParserBase\" has no attribute \"read\"\u001b[39;00m\n\u001b[1;32m   1700\u001b[0m     (\n\u001b[1;32m   1701\u001b[0m         index,\n\u001b[1;32m   1702\u001b[0m         columns,\n\u001b[1;32m   1703\u001b[0m         col_dict,\n\u001b[0;32m-> 1704\u001b[0m     ) \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_engine\u001b[39m.\u001b[39;49mread(  \u001b[39m# type: ignore[attr-defined]\u001b[39;49;00m\n\u001b[1;32m   1705\u001b[0m         nrows\n\u001b[1;32m   1706\u001b[0m     )\n\u001b[1;32m   1707\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m:\n\u001b[1;32m   1708\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclose()\n",
            "File \u001b[0;32m~/Library/Python/3.11/lib/python/site-packages/pandas/io/parsers/c_parser_wrapper.py:234\u001b[0m, in \u001b[0;36mCParserWrapper.read\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m    232\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    233\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlow_memory:\n\u001b[0;32m--> 234\u001b[0m         chunks \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_reader\u001b[39m.\u001b[39;49mread_low_memory(nrows)\n\u001b[1;32m    235\u001b[0m         \u001b[39m# destructive to chunks\u001b[39;00m\n\u001b[1;32m    236\u001b[0m         data \u001b[39m=\u001b[39m _concatenate_chunks(chunks)\n",
            "File \u001b[0;32m~/Library/Python/3.11/lib/python/site-packages/pandas/_libs/parsers.pyx:812\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader.read_low_memory\u001b[0;34m()\u001b[0m\n",
            "File \u001b[0;32m~/Library/Python/3.11/lib/python/site-packages/pandas/_libs/parsers.pyx:873\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._read_rows\u001b[0;34m()\u001b[0m\n",
            "File \u001b[0;32m~/Library/Python/3.11/lib/python/site-packages/pandas/_libs/parsers.pyx:848\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._tokenize_rows\u001b[0;34m()\u001b[0m\n",
            "File \u001b[0;32m~/Library/Python/3.11/lib/python/site-packages/pandas/_libs/parsers.pyx:859\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._check_tokenize_status\u001b[0;34m()\u001b[0m\n",
            "File \u001b[0;32m~/Library/Python/3.11/lib/python/site-packages/pandas/_libs/parsers.pyx:2025\u001b[0m, in \u001b[0;36mpandas._libs.parsers.raise_parser_error\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mParserError\u001b[0m: Error tokenizing data. C error: Expected 1 fields in line 3, saw 2\n"
          ]
        }
      ],
      "source": [
        "datapath = \"/Users/jacklinewambui/Desktop/finalProject/CS72 Project_Emails_training_dataset.csv\"\n",
        "df= pd.read_csv(datapath, encoding=\"'latin1\")\n",
        "df = df.dropna()\n",
        "df.isnull().values.any()\n",
        "df = df.sample(frac=1)\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 146,
      "metadata": {},
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'df' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[146], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mdf\u001b[49m\u001b[38;5;241m.\u001b[39mgroupby([\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCategory\u001b[39m\u001b[38;5;124m'\u001b[39m])\u001b[38;5;241m.\u001b[39msize()\u001b[38;5;241m.\u001b[39mplot\u001b[38;5;241m.\u001b[39mbar()\n",
            "\u001b[0;31mNameError\u001b[0m: name 'df' is not defined"
          ]
        }
      ],
      "source": [
        "df.groupby(['Category']).size().plot.bar()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def preprocess_text(sen): \n",
        "    # Removing html tags\n",
        "    sentence = remove_tags(sen)\n",
        "\n",
        "    # Remove punctuations and numbers\n",
        "    sentence = re.sub('[^a-zA-Z]', ' ', sentence)\n",
        "\n",
        "    # Single character removal\n",
        "    sentence = re.sub(r\"\\s+[a-zA-Z]\\s+\", ' ', sentence)\n",
        "\n",
        "    # Removing multiple spaces\n",
        "    sentence = re.sub(r'\\s+', ' ', sentence)\n",
        "\n",
        "    return sentence"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "TAG_RE = re.compile(r'<[^>]+>')\n",
        "\n",
        "def remove_tags(text):\n",
        "    return TAG_RE.sub('', text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "emails = []\n",
        "sentences = list(df[\"Emails\"])\n",
        "for sen in sentences:\n",
        "  emails.append(preprocess_text(sen))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "tokenizer = BertTokenizer.from_pretrained('bert-base-cased')\n",
        "labels = {'Related': 0,\n",
        "          'Unrelated': 1\n",
        "          }\n",
        "\n",
        "class Dataset(torch.utils.data.Dataset):\n",
        "\n",
        "    def __init__(self, df):\n",
        "\n",
        "        self.labels = [labels[label] for label in df['Category']]\n",
        "        self.texts = [tokenizer(text, \n",
        "                               padding='max_length', max_length = 512, truncation=True,\n",
        "                                return_tensors=\"pt\") for text in df['Emails']]\n",
        "\n",
        "    def classes(self):\n",
        "        return self.labels\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "    def get_batch_labels(self, idx):\n",
        "        # Fetch a batch of labels\n",
        "        return np.array(self.labels[idx])\n",
        "\n",
        "    def get_batch_texts(self, idx):\n",
        "        # Fetch a batch of inputs\n",
        "        return self.texts[idx]\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "\n",
        "        batch_texts = self.get_batch_texts(idx)\n",
        "        batch_y = self.get_batch_labels(idx)\n",
        "\n",
        "        return batch_texts, batch_y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class BertClassifier(nn.Module):\n",
        "\n",
        "    def __init__(self, dropout=0.5):\n",
        "\n",
        "        super(BertClassifier, self).__init__()\n",
        "\n",
        "        self.bert = BertModel.from_pretrained('bert-base-cased')\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.linear = nn.Linear(768, 5)\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "    def forward(self, input_id, mask):\n",
        "\n",
        "        _, pooled_output = self.bert(input_ids= input_id, attention_mask=mask,return_dict=False)\n",
        "        dropout_output = self.dropout(pooled_output)\n",
        "        linear_output = self.linear(dropout_output)\n",
        "        final_layer = self.relu(linear_output)\n",
        "\n",
        "        return final_layer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "np.random.seed(112)\n",
        "df_train, df_val, df_test = np.split(df.sample(frac=1, random_state=42), \n",
        "                                     [int(.8*len(df)), int(.9*len(df))])\n",
        "\n",
        "print(len(df_train),len(df_val), len(df_test))\n",
        "print(type(df_train))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def train(model, train_data, val_data, learning_rate, epochs):\n",
        "\n",
        "    train, val = Dataset(train_data), Dataset(val_data)\n",
        "\n",
        "    train_dataloader = torch.utils.data.DataLoader(train, batch_size=2, shuffle=True)\n",
        "    val_dataloader = torch.utils.data.DataLoader(val, batch_size=2)\n",
        "\n",
        "    use_cuda = torch.cuda.is_available()\n",
        "    device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
        "\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = Adam(model.parameters(), lr= learning_rate)\n",
        "\n",
        "    if use_cuda:\n",
        "\n",
        "            model = model.cuda()\n",
        "            criterion = criterion.cuda()\n",
        "\n",
        "    for epoch_num in range(epochs):\n",
        "\n",
        "            total_acc_train = 0\n",
        "            total_loss_train = 0\n",
        "\n",
        "            for train_input, train_label in tqdm(train_dataloader):\n",
        "\n",
        "                train_label = train_label.to(device)\n",
        "                mask = train_input['attention_mask'].to(device)\n",
        "                input_id = train_input['input_ids'].squeeze(1).to(device)\n",
        "\n",
        "                output = model(input_id, mask)\n",
        "                \n",
        "                batch_loss = criterion(output, train_label.long())\n",
        "                total_loss_train += batch_loss.item()\n",
        "                \n",
        "                acc = (output.argmax(dim=1) == train_label).sum().item()\n",
        "                total_acc_train += acc\n",
        "\n",
        "                model.zero_grad()\n",
        "                batch_loss.backward()\n",
        "                optimizer.step()\n",
        "            \n",
        "            total_acc_val = 0\n",
        "            total_loss_val = 0\n",
        "\n",
        "            with torch.no_grad():\n",
        "\n",
        "                for val_input, val_label in val_dataloader:\n",
        "\n",
        "                    val_label = val_label.to(device)\n",
        "                    mask = val_input['attention_mask'].to(device)\n",
        "                    input_id = val_input['input_ids'].squeeze(1).to(device)\n",
        "\n",
        "                    output = model(input_id, mask)\n",
        "\n",
        "                    batch_loss = criterion(output, val_label.long())\n",
        "                    total_loss_val += batch_loss.item()\n",
        "                    \n",
        "                    acc = (output.argmax(dim=1) == val_label).sum().item()\n",
        "                    total_acc_val += acc\n",
        "            \n",
        "            print(\n",
        "                f'Epochs: {epoch_num + 1} | Train Loss: {total_loss_train / len(train_data): .3f} \\\n",
        "                | Train Accuracy: {total_acc_train / len(train_data): .3f} \\\n",
        "                | Val Loss: {total_loss_val / len(val_data): .3f} \\\n",
        "                | Val Accuracy: {total_acc_val / len(val_data): .3f}')\n",
        "                  \n",
        "EPOCHS = 5\n",
        "model = BertClassifier()\n",
        "LR = 1e-6\n",
        "              \n",
        "train(model, df_train, df_val, LR, EPOCHS)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def evaluate(model, test_data):\n",
        "\n",
        "    test = Dataset(test_data)\n",
        "\n",
        "    test_dataloader = torch.utils.data.DataLoader(test, batch_size=2)\n",
        "\n",
        "    use_cuda = torch.cuda.is_available()\n",
        "    device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
        "\n",
        "    y_preds = []\n",
        "    y_trues = []\n",
        "\n",
        "    if use_cuda:\n",
        "        model = model.cuda()\n",
        "\n",
        "    total_acc_test = 0\n",
        "    with torch.no_grad():\n",
        "\n",
        "        for test_input, test_label in test_dataloader:\n",
        "\n",
        "              test_label = test_label.to(device)\n",
        "\n",
        "              y_trues.append(test_label)\n",
        "\n",
        "              mask = test_input['attention_mask'].to(device)\n",
        "              input_id = test_input['input_ids'].squeeze(1).to(device)\n",
        "\n",
        "              output = model(input_id, mask)\n",
        "              y_preds.append(output.argmax(dim=1))\n",
        "\n",
        "              acc = (output.argmax(dim=1) == test_label).sum().item()\n",
        "              total_acc_test += acc\n",
        "    \n",
        "    print(f'Test Accuracy: {total_acc_test / len(test_data): .3f}')\n",
        "    \n",
        "    # confusion_matrix = metrics.confusion_matrix(y_trues, y_preds, labels=[\"Related\", \"Unrelated\"])\n",
        "    # print(confusion_matrix)\n",
        "    \n",
        "evaluate(model, df_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def preprocessToken(input_text, tokenizer):\n",
        "  return tokenizer.encode_plus(input_text, \n",
        "                               add_special_tokens = True,\n",
        "                               max_length = 512,\n",
        "                               padding='longest',\n",
        "                               return_attention_mask = True,\n",
        "                               return_tensors = 'pt'\n",
        "                               )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 103,
      "metadata": {
        "id": "uZ5NcLjewsB-"
      },
      "outputs": [],
      "source": [
        "def predict(model, userinput):\n",
        "  input_ids = []\n",
        "  attention_mask = []\n",
        "  encoding = preprocessToken(userinput, tokenizer)\n",
        "\n",
        "  input_ids.append(encoding['input_ids'])\n",
        "  attention_mask.append(encoding['attention_mask'])\n",
        "  input_ids = torch.cat(input_ids, dim = 0)\n",
        "  attention_mask = torch.cat(attention_mask, dim = 0)\n",
        "\n",
        "  output = model(input_ids, attention_mask)\n",
        "  prediction = 'Related' if np.argmax(output.detach().numpy()).flatten()==0 else 'Unrelated'\n",
        "  # print(\"Tensor is: \", output.argmax(dim=1))\n",
        "  print(\"This message is: \", prediction)\n",
        "\n",
        "  return prediction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4-L5iJSExQn4"
      },
      "source": [
        "GET THE EXTRACTED INFORMATION FROM PRETRAINED STANFORD NER TAGGER "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 147,
      "metadata": {
        "id": "wMbKyfRQxKv5"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'testStanfordNERTTagger' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[147], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# python \"NER.ipynb\"\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m classified_text\u001b[38;5;241m=\u001b[39m \u001b[43mtestStanfordNERTTagger\u001b[49m(userInput)\n",
            "\u001b[0;31mNameError\u001b[0m: name 'testStanfordNERTTagger' is not defined"
          ]
        }
      ],
      "source": [
        "# python \"NER.ipynb\"\n",
        "\n",
        "classified_text= testStanfordNERTTagger(userInput)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-GpgkyyKx1Th"
      },
      "source": [
        "Test the BERT Classifier and StanfordNER Model a single email from the gmail account: jackline.w.gathoni.24@dartmouth.edu"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 117,
      "metadata": {
        "id": "7Y8yEGqhxoYN"
      },
      "outputs": [
        {
          "ename": "AttributeError",
          "evalue": "module 'Training' has no attribute 'preprocess_text'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[117], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mTraining\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mTraining\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpreprocess_text\u001b[49m(userInput)\n\u001b[1;32m      4\u001b[0m pred \u001b[38;5;241m=\u001b[39m predict(model, userInput)\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# if(pred == 'Related'):\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m#   //\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m#   classified_text = testStanfordNERTTagger(userInput)\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m#   print(classified_text)\u001b[39;00m\n",
            "\u001b[0;31mAttributeError\u001b[0m: module 'Training' has no attribute 'preprocess_text'"
          ]
        }
      ],
      "source": [
        "import Training\n",
        "\n",
        "model = Training.preprocess_text(userInput)\n",
        "pred = predict(model, userInput)\n",
        "\n",
        "\n",
        "# if(pred == 'Related'):\n",
        "#   //\n",
        "#   classified_text = testStanfordNERTTagger(userInput)\n",
        "#   print(classified_text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2OaeB1zEy_KV"
      },
      "source": [
        "Parse the time from the classified text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d13A7j3ry6lX"
      },
      "outputs": [],
      "source": [
        "import datetime\n",
        "from zoneinfo import ZoneInfo\n",
        "import re\n",
        "\n",
        "def getTime(classified_text):\n",
        "  time = []\n",
        "  specialTimes={}\n",
        "  specialTimes[\"noon\"]= \"12\"\n",
        "  specialTimes[\"midnight\"]= \"00\"\n",
        "\n",
        "\n",
        "  for item in classified_text:\n",
        "    if(item[1] == 'TIME'):\n",
        "      \n",
        "      if not any(chr.isdigit() for chr in item[0]): \n",
        "        continue\n",
        "\n",
        "      #regex for parsing time\n",
        "      # expression results in five groups\n",
        "      reTime = r\"([0-9]*:*[0-9]*)(.*)(-|to)([0-9]*:*[0-9]*)(.*)\"\n",
        "      timeBreakdown = re.search(reTime, item[0], re.IGNORECASE)\n",
        "\n",
        "\n",
        "      # Group1 = start_time\n",
        "      # Group2 = am/pm/ other exceptions like noon, midnight\n",
        "      # Group3 = -/to\n",
        "      # Group4 = end_time \n",
        "      # Group5 = am/pm/other exceptions like noon, midnight\n",
        "      if timeBreakdown != None:\n",
        "        startTime = timeBreakdown.group(1)\n",
        "        startPeriod = timeBreakdown.group(2)\n",
        "        to = timeBreakdown.group(3)\n",
        "        endTime = timeBreakdown.group(4)\n",
        "        endPeriod =  timeBreakdown.group(5)\n",
        "\n",
        "\n",
        "        if startPeriod in specialTimes:\n",
        "          startTime = specialTimes[startPeriod]\n",
        "        if endPeriod in specialTimes:\n",
        "          endTime = specialTimes[endPeriod]\n",
        "\n",
        "\n",
        "        def splitTime(time, period):\n",
        "          timeDiv = time.split(':')\n",
        "          hours= int(timeDiv[0])\n",
        "         \n",
        "          if period == 'p' or period =='pm':\n",
        "            hours+=12\n",
        "          if len(timeDiv) == 2:\n",
        "            minutes = int(timeDiv[1])\n",
        "          else:\n",
        "            minutes = 0;\n",
        "          return hours, minutes\n",
        "     \n",
        "\n",
        "        start_time = datetime.time(splitTime(startTime, startPeriod)[0], (splitTime(startTime,startPeriod)[1]), tzinfo = ZoneInfo('US/Eastern'))\n",
        "        end_time = datetime.time(splitTime(endTime, endPeriod)[0], (splitTime(endTime, endPeriod)[1]), tzinfo = ZoneInfo('US/Eastern'))\n",
        "        return(start_time, end_time)\n",
        "\n",
        "  return None"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sYQIOhjzyPbb"
      },
      "source": [
        "GET CREDENTIALS FROM GOOGLE CALENDAR API"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 112,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Getting the upcoming 10 events...\n"
          ]
        },
        {
          "ename": "TypeError",
          "evalue": "combine() argument 2 must be datetime.time, not int",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[112], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mScheduling\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcheckAvailability\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m12\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m<string>:72\u001b[0m, in \u001b[0;36mcheckAvailability\u001b[0;34m(start_time, end_time)\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: combine() argument 2 must be datetime.time, not int"
          ]
        }
      ],
      "source": [
        "start_time = getTime(classified_text)[0]\n",
        "end_time = getTime(classified_text)[1]\n",
        "available = Scheduling.checkAvailability(start_time, end_time)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bySG9vkD38uP"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "import Email\n",
        "\n",
        "\n",
        "# Extracts the sender's email from sender's information: .<(.*)>\n",
        "reEmail= r\".<(.*)>\"\n",
        "service = Email.getService()\n",
        "emailDetails = Email.getEmails(service)\n",
        "\n",
        "\n",
        "destination = re.search(reEmail, emailDetails[\"Sender\"], re.IGNORECASE).group(1)\n",
        "print(\"destination email is: \", destination)\n",
        "recipients= re.search(reEmail, emailDetails[\"Recipients\"], re.IGNORECASE)\n",
        "if recipients:\n",
        "  recipientEmail = recipients.group(1)\n",
        "print(\"other recipient email is: \", recipients)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n5II-8tnzid8"
      },
      "outputs": [],
      "source": [
        "\n",
        "#Sends email responding to the sender if available for the time window of the shift dropped\n",
        "\n",
        "from base64 import urlsafe_b64encode\n",
        "from email.mime.text import MIMEText\n",
        "\n",
        "\n",
        "def build_message(destination, obj, body):\n",
        "    message = MIMEText(body)\n",
        "\n",
        "    message['to'] = destination\n",
        "    message['from'] = \"jackline.w.gathoni.24@dartmouth.edu\"\n",
        "    message['subject'] = obj\n",
        "    message['In-Reply-To']= destination\n",
        "    if recipientEmail != \"jackline.w.gathoni.24@dartmouth.edu\":\n",
        "      message['References']= recipients\n",
        "\n",
        "    return {'raw': urlsafe_b64encode(message.as_bytes()).decode()}\n",
        "\n",
        "def send_message(service, destination, obj, body):\n",
        "    return service.users().messages().send(\n",
        "      userId=\"me\",\n",
        "      body=build_message(destination, obj, body)\n",
        "    ).execute()\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if available:\n",
        "    # test send email\n",
        "    send_message(service, destination, emailDetails[\"Subject\"], \n",
        "                \"I can\",\n",
        "    )"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.11.3 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.3"
    },
    "vscode": {
      "interpreter": {
        "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
